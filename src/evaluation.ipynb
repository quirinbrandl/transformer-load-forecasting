{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide paths to the directory with experimental results and cleaned preprocessed data for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = '../output/predictions/'\n",
    "data_real_path = '../data/electricity_cleaned_preprocessed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "MAE = 'MAE'\n",
    "MAPE = 'MAPE'\n",
    "SMAPE = 'sMAPE'\n",
    "MSE = 'MSE'\n",
    "R2 = 'R2'\n",
    "MASE = 'MASE'\n",
    "\n",
    "ARIMA = 'ARIMA'\n",
    "PROPHET = 'Prophet'\n",
    "SVR = 'SVR'\n",
    "RF = 'RF'\n",
    "LSTM = 'LSTM'\n",
    "TRANSFORMER = 'Transformer'\n",
    "\n",
    "OFFICE = 'Office'\n",
    "EDUCATION = 'Education'\n",
    "LODGING = 'Lodging/resedential'\n",
    "PUBLIC = 'Public services'\n",
    "ASSEMBLY = 'Entertainment/public assembly'\n",
    "INDUSTRIAL = 'Manufacturing/industrial'\n",
    "PARKING = 'Parking'\n",
    "SERVICES = 'Services'\n",
    "FOOD = 'Food sales and service'\n",
    "OTHER = 'Other'\n",
    "\n",
    "\n",
    "metric_columns = [MAE, SMAPE, MAPE, MSE, R2, MASE]\n",
    "\n",
    "model_name_mapper = {\n",
    "    'arima': ARIMA,\n",
    "    'prophet': PROPHET,\n",
    "    'svr': SVR,\n",
    "    'rf': RF,\n",
    "    'lstm': LSTM,\n",
    "    'transformer': TRANSFORMER\n",
    "}\n",
    "\n",
    "type_name_mapper = {\n",
    "    'office': OFFICE,\n",
    "    'education': EDUCATION,\n",
    "    'lodging': LODGING,\n",
    "    'public': PUBLIC,\n",
    "    'assembly': ASSEMBLY,\n",
    "    'industrial': INDUSTRIAL,\n",
    "    'parking': PARKING,\n",
    "    'services': SERVICES,\n",
    "    'food': FOOD,\n",
    "    'other': OTHER\n",
    "}\n",
    "\n",
    "name_mapper = {**model_name_mapper, **type_name_mapper}\n",
    "\n",
    "# model_order = [AR, ARIMA, ARIMA_NOCOVARS,ARIMA_NOSEASON, PROPHET, SVR, RF, LSTM, TRANSFORMER]\n",
    "MODEL_ORDER = [ARIMA, PROPHET, SVR, RF, LSTM, TRANSFORMER]\n",
    "TYPE_ORDER = [OFFICE, EDUCATION, LODGING, PUBLIC, ASSEMBLY, INDUSTRIAL, PARKING, SERVICES, FOOD, OTHER]\n",
    "\n",
    "# Based on colorblind\n",
    "COLOR_PALETTE = []\n",
    "for color in [0,1,2,3,4,7]:\n",
    "    COLOR_PALETTE.append(sns.color_palette(\"colorblind\")[color])\n",
    "COLOR_PALETTE\n",
    "\n",
    "def filter_relevant_models(df):\n",
    "    return df[df['model'].isin(MODEL_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_predictions = pd.read_csv(os.path.join(predictions_dir, 'predictions.csv'), parse_dates=True, index_col=['timestamp', 'model', 'training_length'])\n",
    "data_real = pd.read_csv(data_real_path, parse_dates=True, index_col='timestamp')\n",
    "data_real = data_real.loc[data_real.index, data_real.columns.isin(data_predictions.columns)]\n",
    "data_times = pd.read_csv(os.path.join(predictions_dir, 'times.csv'))\n",
    "data_times = data_times.replace(name_mapper)\n",
    "\n",
    "data_predictions.columns.name = 'building'\n",
    "data_predictions = data_predictions.stack(future_stack=True)\n",
    "data_predictions.name = 'predicted'\n",
    "data_predictions = data_predictions.reset_index()\n",
    "\n",
    "data_real['model'] = 'real'\n",
    "data_real.columns.name = 'building'\n",
    "data_real = data_real.stack(future_stack=True)\n",
    "data_real.name = 'real'\n",
    "data_real = data_real.reset_index()\n",
    "\n",
    "data = data_predictions.merge(data_real, on=['timestamp', 'building'])\n",
    "data = data.replace(name_mapper)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create naive forecaster\n",
    "data['naive'] = data.groupby(['model', 'building', 'training_length'])['real'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_increase(df, metric_column_name):\n",
    "    return df.apply(lambda x: x[metric_column_name]/df[df['model'] == x['model']].loc[14, metric_column_name], axis=1)\n",
    "\n",
    "def smape(y_true, y_pred) -> float: \n",
    "    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true))/2))*100\n",
    "\n",
    "def mase(y_true, y_pred, naive) -> float: \n",
    "    return np.mean(np.abs(y_pred - y_true) / np.mean(np.abs(naive-y_true)))\n",
    "\n",
    "def apply_metric(dataframe, metric_function):\n",
    "    if metric_function == mase:\n",
    "        return metric_function(y_true=dataframe['real'], y_pred=dataframe['predicted'], naive=dataframe['naive'])\n",
    "    return metric_function(y_true=dataframe['real'], y_pred=dataframe['predicted'])\n",
    "\n",
    "def calculate_metrics(dataframe):\n",
    "    metrics = pd.DataFrame()\n",
    "    metrics[MAE] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted']].apply(lambda x:apply_metric(x, mean_absolute_error))\n",
    "    metrics[MAPE] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted']].apply(lambda x: apply_metric(x, mean_absolute_percentage_error))\n",
    "    metrics[SMAPE] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted']].apply(lambda x: apply_metric(x, smape))\n",
    "    metrics[MSE] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted']].apply(lambda x: apply_metric(x, mean_squared_error))\n",
    "    metrics[R2] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted']].apply(lambda x: apply_metric(x, r2_score))\n",
    "    metrics[MASE] = dataframe.groupby(['model', 'building', 'training_length'])[['real', 'predicted', 'naive']].apply(lambda x: apply_metric(x, mase))\n",
    "    return metrics\n",
    "\n",
    "metrics = calculate_metrics(data)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated by model\n",
    "\n",
    "by_model = metrics.groupby(['model', 'training_length'])[metric_columns].agg('mean').reset_index().set_index('training_length')\n",
    "by_model['mase_percentage'] = calculate_increase(by_model, metric_column_name=MASE) * 100\n",
    "by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_model.groupby(['training_length']).agg({'MASE': 'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def metrics_visualization(data, metric_column, aggregation_function='mean', **kwargs):\n",
    "    agg_metric_by_model = data.groupby(['model', 'training_length'])[metric_columns].agg(aggregation_function).reset_index().set_index('training_length')\n",
    "    ax = sns.lineplot(data=agg_metric_by_model, x='training_length', y=metric_column, hue='model', hue_order=MODEL_ORDER)\n",
    "    ax.legend(title=None)\n",
    "    ax.set_xticks(agg_metric_by_model.index.unique())\n",
    "    ax.set_xlabel('Training data size (days)')\n",
    "    return ax\n",
    "\n",
    "def metrics_visualization_bar(data, metric_column, aggregation_function='mean', **kwargs):\n",
    "    agg_metric_by_model = data.groupby(['model', 'training_length'])[metric_columns].agg(aggregation_function).reset_index().set_index('training_length')\n",
    "    ax = sns.barplot(data=agg_metric_by_model, x='training_length', y=metric_column, hue='model', hue_order=MODEL_ORDER)\n",
    "    ax.legend(title=None)\n",
    "    ax.set_xlabel('Training data size (days)')\n",
    "    return ax\n",
    "\n",
    "def metrics_visualization_box(data, metric_column, palette=None, **kwargs):\n",
    "    palette = palette if palette else COLOR_PALETTE\n",
    "    agg_metric_by_model = data.reset_index().set_index('training_length')\n",
    "    ax = sns.boxplot(data=agg_metric_by_model, x='training_length', y=metric_column, hue='model', showfliers=False,\n",
    "                      hue_order=MODEL_ORDER, palette=palette, showmeans=True,\n",
    "                        meanprops={'marker':'o','markerfacecolor':'white','markeredgecolor':'black'})\n",
    "    ax.legend(title=None)\n",
    "    ax.set_xlabel('Training data size (days)')\n",
    "    sns.despine(left=True)\n",
    "    return ax\n",
    "\n",
    "def metrics_visualization_box_alt(data, metric_column, palette=None, type_order=None, width=None, **kwargs):\n",
    "    palette = palette if palette else 'Set3'\n",
    "    type_order = type_order if type_order else TYPE_ORDER\n",
    "    width = width if width else 0.8\n",
    "    agg_metric_by_model = data.reset_index().set_index('training_length')      \n",
    "    ax = sns.boxplot(data=agg_metric_by_model, x='training_length', y=metric_column, hue='type', showfliers=False,\n",
    "                      hue_order=type_order, palette=palette, width=width, showmeans=True,\n",
    "                        meanprops={'marker':'o','markerfacecolor':'white','markeredgecolor':'black', 'markersize':5})\n",
    "    ax.legend(title=None)\n",
    "    ax.set_xlabel('Training data size (days)')\n",
    "    sns.despine(left=True)\n",
    "    return ax\n",
    "\n",
    "def heatmap_visualization(data, metric_column, aggregation_function='mean', annot=False, **kwargs):\n",
    "    day_metrics_agg = data.groupby(['training_length', 'day'])[metric_column].agg(aggregation_function).reset_index()\n",
    "    day_metrics_agg = day_metrics_agg.pivot(index='training_length', columns='day', values=metric_column)\n",
    "    ax = sns.heatmap(data=day_metrics_agg, cmap=\"flare\", annot=annot, cbar=False)\n",
    "    ax.set_xlabel('Day in forecasting horizon')\n",
    "    ax.set_ylabel('Training data size (days)')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "ax = metrics_visualization_box(metrics, MASE)\n",
    "ax.set_ylabel('MASE')\n",
    "plt.tight_layout()\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_detailed = metrics.reset_index()\n",
    "metrics_detailed[['location', 'type', 'building']] = metrics_detailed['building'].str.split('_', expand=True)\n",
    "metrics_detailed = metrics_detailed.set_index('training_length')\n",
    "metrics_detailed = metrics_detailed.replace(name_mapper)\n",
    "metrics_detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(metrics_detailed, col='location', aspect=2)\n",
    "g.map_dataframe(metrics_visualization_box, metric_column=MASE)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "g = sns.FacetGrid(metrics_detailed, col='model', aspect=2, sharey=False, col_wrap=2, col_order=MODEL_ORDER)\n",
    "g.map_dataframe(metrics_visualization_box_alt, metric_column=MASE, type_outliers=['Services'])\n",
    "g.add_legend()\n",
    "sns.move_legend(g, 'lower center',  bbox_to_anchor=(0.4, -0.08), ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_adjusted = metrics_detailed[metrics_detailed.type.isin(['Services'])]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "g = sns.FacetGrid(metrics_adjusted, col='model', aspect=0.7, sharey=False, col_wrap=3, col_order=MODEL_ORDER).tight_layout()\n",
    "g.map_dataframe(metrics_visualization_box_alt, metric_column=MASE, hue_order=TYPE_ORDER[-1], type_order=['Services'], palette=[COLOR_PALETTE[-1]])\n",
    "g.add_legend()\n",
    "sns.move_legend(g, 'lower center',  bbox_to_anchor=(0.5, -0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_adjusted = metrics_detailed[~metrics_detailed.type.isin(['Services'])]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "g = sns.FacetGrid(metrics_adjusted, col='model', aspect=2, sharey=False, col_wrap=3, col_order=MODEL_ORDER).tight_layout()\n",
    "g.map_dataframe(metrics_visualization_box_alt, metric_column=MASE, type_outliers=['Services'], hue_order=[SERVICES])\n",
    "g.add_legend()\n",
    "sns.move_legend(g, 'lower center',  bbox_to_anchor=(0.4, -0.08), ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_days = [pd.Timestamp(day).date() for day in ('2017-04-03', '2017-08-07', '2017-10-02', '2017-12-11')]\n",
    "\n",
    "data['day'] = data.apply(lambda row: min([abs((row.name.date() - first_date).days) for first_date in first_days]) + 1, axis=1)\n",
    "day_metrics = data.groupby('day').apply(calculate_metrics, include_groups=False).reset_index()\n",
    "day_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "g = sns.FacetGrid(day_metrics, col='model', aspect=1.1, col_wrap=2, col_order=MODEL_ORDER, height=2.8).tight_layout()\n",
    "g.map_dataframe(heatmap_visualization, metric_column=MASE, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Time Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_visualization(df):\n",
    "    ax = sns.lineplot(data=df, x='training_length', y='process_time_h', hue='model', hue_order=MODEL_ORDER, palette=COLOR_PALETTE)\n",
    "    ax.legend(title=None, frameon=False)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.set_xticks(df.index.unique())\n",
    "    ax.set_xlabel('Training data size (days)')\n",
    "    ax.set_ylabel('CPU execution time (hours)')\n",
    "    sns.despine(bottom=True)\n",
    "    return ax\n",
    "\n",
    "def times_visualization_bar(df, log_scale=True):\n",
    "    ax = sns.barplot(data=df, x='process_time_h', y='training_length', hue='model', hue_order=MODEL_ORDER, palette=COLOR_PALETTE, orient='y')\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        datapoints = container.datavalues\n",
    "        percentages = np.round(datapoints/datapoints[0]*100)\n",
    "        labels = [f'{int(number)} %' for number in percentages]\n",
    "        ax.bar_label(container, labels=labels, color=COLOR_PALETTE[i] , fontsize=10, padding=5)\n",
    "    ax.legend(title=None, frameon=False)\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.set_ylabel('Training data size (days)')\n",
    "    ax.set_xlabel('CPU execution time (hours)')\n",
    "    if log_scale:\n",
    "        ax.set_xscale(\"log\")\n",
    "    sns.despine(bottom=True)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def times_increase_visualization_bar(df, log_scale=True):\n",
    "    ax = sns.barplot(data=df, y='training_length', x='process_time_increase_percentage', hue='model', hue_order=MODEL_ORDER, palette=COLOR_PALETTE, orient='y')\n",
    "    ax.legend(title=None, frameon=True)\n",
    "    # sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    ax.set_ylabel('Training data size (days)')\n",
    "    ax.set_xlabel('CPU execution time increase (percentage)')\n",
    "    if log_scale:\n",
    "        ax.set_xscale(\"log\")\n",
    "    sns.despine(bottom=True)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_times_by_model = data_times.groupby(['model', 'training_length']).sum().reset_index().set_index('training_length')\n",
    "sum_times_by_model[['process_time_h', 'thread_time_h', 'wall_time_h']] =  sum_times_by_model[['process_time_ns', 'thread_time_ns', 'wall_time_ns']] / (3600 * 10**9)\n",
    "sum_times_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_times_by_model['process_time_increase_percentage'] = calculate_increase(sum_times_by_model, metric_column_name='process_time_h') * 100\n",
    "sum_times_by_model['thread_time_increase_percentage'] = calculate_increase(sum_times_by_model, metric_column_name='thread_time_h') * 100\n",
    "sum_times_by_model['wall_time_increase_percentage'] = calculate_increase(sum_times_by_model, metric_column_name='wall_time_h') * 100\n",
    "sum_times_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "ax = times_visualization_bar(sum_times_by_model, log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "sns.set_style(\"whitegrid\")\n",
    "times_visualization_bar(sum_times_by_model, log_scale=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "ax = times_increase_visualization_bar(sum_times_by_model, log_scale=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Performance Metrics and Execution Time Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def combined_visualization(df, performance_metric, time_metric='process_time_h'):\n",
    "    ax = sns.scatterplot(x=time_metric, y=performance_metric, data=df, hue='model', hue_order=MODEL_ORDER, palette=COLOR_PALETTE)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metric_by_model = metrics.groupby(['model', 'training_length'])[metric_columns].agg('mean').reset_index().set_index('training_length')\n",
    "combined_by_model = sum_times_by_model.reset_index().merge(mean_metric_by_model.reset_index(), on=['training_length', 'model']).set_index('training_length')\n",
    "combined_by_model = filter_relevant_models(combined_by_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "ax = combined_visualization(combined_by_model, time_metric='process_time_h', performance_metric=MASE)\n",
    "ax.set_ylabel('MASE')\n",
    "ax.set_xlabel('CPU execution time (hours)')\n",
    "ax.legend(title=None, frameon=False)\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Create inset axes\n",
    "axins = inset_axes(ax, width=\"20%\", height=\"40%\", loc=1)\n",
    "\n",
    "# Plot zoomed data in inset\n",
    "sns.scatterplot(ax=axins, x='process_time_h', y=MASE, data=combined_by_model, hue='model', hue_order=MODEL_ORDER, palette=COLOR_PALETTE)\n",
    "\n",
    "# Set limits for zoomed region\n",
    "x1, x2, y1, y2 = -0.2, 0.5, 2, 3  # Adjust these values as needed\n",
    "axins.set_xlim(x1, x2)\n",
    "axins.set_ylim(y1, y2)\n",
    "axins.set_xlabel(None)\n",
    "axins.set_ylabel(None)\n",
    "axins.get_legend().remove()\n",
    "\n",
    "# Add zoom indicator\n",
    "mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "\n",
    "ax\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"results_time_mase.pdf\")\n",
    "plt.savefig(\"results_time_mase.png\", dpi=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = combined_visualization(combined_by_model, time_metric='process_time_h', performance_metric=MASE)\n",
    "ax.set_ylabel('MASE')\n",
    "ax.set_xlabel('CPU execution time (hours)')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = combined_visualization(combined_by_model, time_metric='process_time_increase_percentage', performance_metric=MASE)\n",
    "ax.set_ylabel('MASE')\n",
    "ax.set_xlabel('CPU execution time increase (percentage)')\n",
    "ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renergetic-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
